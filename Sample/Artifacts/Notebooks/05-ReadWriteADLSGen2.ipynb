{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Access data on Azure Data Lake Storage Gen2 (ADLS Gen2) with Synapse Spark\n",
        "\n",
        "Azure Data Lake Storage Gen2 (ADLS Gen2) is used as the storage account associated with a Synapse workspace. A synapse workspace can have a default ADLS Gen2 storage account and additional linked storage accounts. \n",
        "\n",
        "You can access data on ADLS Gen2 with Synapse Spark via following URL:\n",
        "    \n",
        "    abfss://<container_name>@<storage_account_name>.dfs.core.windows.net/<path>\n",
        "\n",
        "This notebook provides examples of how to read data from ADLS Gen2 account into a Spark context and how to write the output of Spark jobs directly into an ADLS Gen2 location.\n",
        "\n",
        "## Pre-requisites\n",
        "Synapse leverage AAD pass-through to access any ADLS Gen2 account (or folder) to which you have a **Blob Storage Contributor** permission. No credentials or access token is required. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load a sample data\n",
        "\n",
        "Let's first load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) of last 6 months from Azure Open datasets as a sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 11,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:45:52.5817535Z",
              "execution_start_time": "2021-02-23T05:46:37.6456674Z",
              "execution_finish_time": "2021-02-23T05:46:58.0341127Z"
            },
            "text/plain": "StatementMeta(small, 11, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 1,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from azureml.opendatasets import PublicHolidays\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil import parser\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "\n",
        "end_date = datetime.today()\n",
        "start_date = datetime.today() - relativedelta(months=6)\n",
        "hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
        "hol_df = hol.to_spark_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 11,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:45:52.6135931Z",
              "execution_start_time": "2021-02-23T05:46:58.1087993Z",
              "execution_finish_time": "2021-02-23T05:47:02.2130926Z"
            },
            "text/plain": "StatementMeta(small, 11, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+---------------+-------------------------+-------------------------+-------------+-----------------+-------------------+\n|countryOrRegion|holidayName              |normalizeHolidayName     |isPaidTimeOff|countryRegionCode|date               |\n+---------------+-------------------------+-------------------------+-------------+-----------------+-------------------+\n|Ukraine        |День незалежності України|День незалежності України|null         |UA               |2020-08-24 00:00:00|\n|Norway         |Søndag                   |Søndag                   |null         |NO               |2020-08-30 00:00:00|\n|Sweden         |Söndag                   |Söndag                   |null         |SE               |2020-08-30 00:00:00|\n|England        |Late Summer Bank Holiday |Late Summer Bank Holiday |null         |null             |2020-08-31 00:00:00|\n|Isle of Man    |Late Summer Bank Holiday |Late Summer Bank Holiday |null         |IM               |2020-08-31 00:00:00|\n+---------------+-------------------------+-------------------------+-------------+-----------------+-------------------+\nonly showing top 5 rows"
          },
          "execution_count": 2,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Display 5 rows\n",
        "hol_df.show(5, truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write data to the default ADLS Gen2 storage\n",
        "\n",
        "We are going to write the spark dateframe to your default ADLS Gen2 storage account.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Primary storage info\n",
        "account_name = 'fill in your primary account name' # fill in your primary account name\n",
        "container_name = 'fill in your container name' # fill in your container name\n",
        "relative_path = 'fill in your relative folder path' # fill in your relative folder path\n",
        "\n",
        "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\n",
        "print('Primary storage account path: ' + adls_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save a dataframe as Parquet, JSON or CSV\n",
        "If you have a dataframe, you can save it to Parquet or JSON with the .write.parquet(), .write.json() and .write.csv() methods respectively.\n",
        "\n",
        "Dataframes can be saved in any format, regardless of the input format.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 11,
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:45:52.6625871Z",
              "execution_start_time": "2021-02-23T05:47:04.3935322Z",
              "execution_finish_time": "2021-02-23T05:47:06.4352773Z"
            },
            "text/plain": "StatementMeta(small, 11, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "parquet file path: abfss://zhaotest@bdbjtestgen2.dfs.core.windows.net/testholiday.parquet\njson file path： abfss://zhaotest@bdbjtestgen2.dfs.core.windows.net/testholiday.json\ncsv file path: abfss://zhaotest@bdbjtestgen2.dfs.core.windows.net/testholiday.csv"
          },
          "execution_count": 4,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "parquet_path = adls_path + 'holiday.parquet'\n",
        "json_path = adls_path + 'holiday.json'\n",
        "csv_path = adls_path + 'holiday.csv'\n",
        "print('parquet file path: ' + parquet_path)\n",
        "print('json file path： ' + json_path)\n",
        "print('csv file path: ' + csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 11,
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:45:52.6797852Z",
              "execution_start_time": "2021-02-23T05:47:06.5087586Z",
              "execution_finish_time": "2021-02-23T05:47:24.8604777Z"
            },
            "text/plain": "StatementMeta(small, 11, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 5,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "hol_df.write.parquet(parquet_path, mode = 'overwrite')\n",
        "hol_df.write.json(json_path, mode = 'overwrite')\n",
        "hol_df.write.csv(csv_path, mode = 'overwrite', header = 'true')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save a dataframe as text files\n",
        "If you have a dataframe that you want ot save as text file, you must first covert it to an RDD and then save that RDD as a text file.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 11,
              "statement_id": 13,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:47:48.2688679Z",
              "execution_start_time": "2021-02-23T05:47:48.3393164Z",
              "execution_finish_time": "2021-02-23T05:47:50.3895514Z"
            },
            "text/plain": "StatementMeta(small, 11, 13, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "text file path: abfss://zhaotest@bdbjtestgen2.dfs.core.windows.net/testholiday1.txt"
          },
          "execution_count": 11,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Define the text file path\n",
        "text_path = adls_path + 'holiday.txt'\n",
        "print('text file path: ' + text_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 11,
              "statement_id": 14,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:47:51.3721255Z",
              "execution_start_time": "2021-02-23T05:47:51.4505701Z",
              "execution_finish_time": "2021-02-23T05:47:53.486809Z"
            },
            "text/plain": "StatementMeta(small, 11, 14, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "<class 'pyspark.rdd.RDD'>"
          },
          "execution_count": 12,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Covert spark dataframe into RDD \n",
        "hol_RDD = hol_df.rdd\n",
        "type(hol_RDD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you have an RDD, you can convert it to a text file like the following:\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 11,
              "statement_id": 15,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:47:54.9640778Z",
              "execution_start_time": "2021-02-23T05:47:55.0392181Z",
              "execution_finish_time": "2021-02-23T05:47:57.079733Z"
            },
            "text/plain": "StatementMeta(small, 11, 15, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 13,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        " # Save RDD as text file\n",
        "hol_RDD.saveAsTextFile(text_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Read data from the default ADLS Gen2 storage\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a dataframe from parquet files\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 11,
              "statement_id": 16,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:47:58.683066Z",
              "execution_start_time": "2021-02-23T05:47:58.7580394Z",
              "execution_finish_time": "2021-02-23T05:48:00.7927035Z"
            },
            "text/plain": "StatementMeta(small, 11, 16, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 14,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "df_parquet = spark.read.parquet(parquet_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a dataframe from JSON files\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 11,
              "statement_id": 17,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:48:00.2470672Z",
              "execution_start_time": "2021-02-23T05:48:00.859645Z",
              "execution_finish_time": "2021-02-23T05:48:02.8953756Z"
            },
            "text/plain": "StatementMeta(small, 11, 17, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 15,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "df_json = spark.read.json(json_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a dataframe from CSV files\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 11,
              "statement_id": 18,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:48:01.272114Z",
              "execution_start_time": "2021-02-23T05:48:02.9635677Z",
              "execution_finish_time": "2021-02-23T05:48:04.9964063Z"
            },
            "text/plain": "StatementMeta(small, 11, 18, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 16,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "df_csv = spark.read.csv(csv_path, header = 'true')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create an RDD from text file\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 11,
              "statement_id": 19,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:48:02.9788188Z",
              "execution_start_time": "2021-02-23T05:48:05.0621688Z",
              "execution_finish_time": "2021-02-23T05:48:07.0990355Z"
            },
            "text/plain": "StatementMeta(small, 11, 19, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 17,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "text = sc.textFile(text_path)"
      ]
    }
  ]
}